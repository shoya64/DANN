{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'DANN'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e025fd561674>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#import torch.nn.functional as F\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mDANN\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDANNM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mDANN\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDANN_Bi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mDANN\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDANN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'DANN'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import scipy.io\n",
    "#data_被験者番号_実験回数\n",
    "data_1_1 = scipy.io.loadmat('/Users/furukawashouya/SEED/SEED/ExtractedFeatures/1_20131027.mat')\n",
    "\n",
    "#de_被験者番号_実験回数[映画番号-1](62*200*5)\n",
    "de_1_1 = []\n",
    "for i in range(1, 16):\n",
    "    de_1_1.append(data_1_1['de_LDS' + str(i)])\n",
    "\n",
    "#感情ラベル\n",
    "label_data = scipy.io.loadmat('/Users/furukawashouya/SEED/SEED/ExtractedFeatures/label.mat')\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "#import torch.nn.functional as F\n",
    "\n",
    "from DANN import DANNM\n",
    "from DANN import DANN_Bi\n",
    "from DANN import DANN\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "#データ作成\n",
    "def window_slice(data, time_steps):#dataをtimestepsの長さずつに一つずつデータをずらして小分けする\n",
    "    data = np.transpose(data, (1, 0, 2)).reshape(-1, 310)\n",
    "    xs = []\n",
    "    for i in range(data.shape[0] - time_steps):\n",
    "        xs.append(data[i: i + time_steps])\n",
    "    xs = np.concatenate(xs).reshape((len(xs), -1, 310))\n",
    "    return xs\n",
    "\n",
    "def window_slice_MinMax(data, time_steps):\n",
    "    data = np.transpose(data, (1, 0, 2)).reshape(-1, 310)\n",
    "    mm = preprocessing.MinMaxScaler()\n",
    "    data_min_max = mm.fit_transform(data)\n",
    "    xs = []\n",
    "    for i in range(data_min_max.shape[0] - time_steps + 1):\n",
    "        xs.append(data_min_max[i: i + time_steps])\n",
    "    xs = np.concatenate(xs).reshape((len(xs), -1, 310))\n",
    "    return xs\n",
    "\n",
    "def window_slice_MinMax2(data, time_steps):\n",
    "    data = np.transpose(data, (1, 0, 2)).reshape(-1, 310)\n",
    "    mm = preprocessing.MinMaxScaler()\n",
    "    xs = []\n",
    "    for i in range(data.shape[0] - time_steps + 1):\n",
    "        data_min_max = mm.fit_transform(data[i:i+time_steps])\n",
    "        xs.append(data_min_max)\n",
    "    xs = np.concatenate(xs).reshape((len(xs), -1, 310))\n",
    "    return xs\n",
    "\n",
    "\n",
    "\n",
    "def window_slice_Standard(data, time_steps):\n",
    "    data = np.transpose(data, (1, 0, 2)).reshape(-1, 310)\n",
    "    mm = preprocessing.StandardScaler()\n",
    "    data_min_max = mm.fit_transform(data)\n",
    "    xs = []\n",
    "    for i in range(data_min_max.shape[0] - time_steps + 1):\n",
    "        xs.append(data_min_max[i: i + time_steps])\n",
    "    xs = np.concatenate(xs).reshape((len(xs), -1, 310))\n",
    "    return xs\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def setData_imp(data, label_data, time_steps):\n",
    "    label = label_data['label']\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    data_1 = []\n",
    "    data_2 = []\n",
    "    for i in range(15):\n",
    "        if i < 9:\n",
    "            data_1.append(data[i])\n",
    "        else :\n",
    "            data_2.append(data[i])\n",
    "    Y_train, Y_test = train_test_split(label.reshape(15,), test_size = 6, shuffle = False)\n",
    "    for data, label in zip(data_1, list(Y_train)):\n",
    "        X_train.append(window_slice_MinMax(data, time_steps))\n",
    "        y_train.extend([label] * len(X_train[-1]))\n",
    "        \n",
    "    for data, label in zip(data_2, list(Y_test)):\n",
    "        X_test.append(window_slice_MinMax(data, time_steps))\n",
    "        y_test.extend([label] * len(X_test[-1]))\n",
    "    \n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "    plus1 = np.array([1]*y_train.shape[0])\n",
    "    plus2 = np.array([1]*y_test.shape[0])\n",
    "    y_train = y_train + plus1\n",
    "    y_test = y_test + plus2\n",
    "    \n",
    "    \n",
    "    #ドメインラベル,1:train,0:test\n",
    "    domain_train = np.array([1]*y_train.shape[0])\n",
    "    domain_test = np.array([0]*y_test.shape[0])\n",
    "    \n",
    "    return np.concatenate(X_train), np.concatenate(X_test), y_train, y_test, domain_train, domain_test\n",
    "\n",
    "\n",
    "#バッチ生成\n",
    "def mkRandomBatch(train_x, train_t, domain, batch_size):\n",
    "    \n",
    "    idx = np.random.randint(0, train_x.shape[0] - batch_size)\n",
    "    batch_x = train_x[idx:batch_size+idx, :, :]\n",
    "    batch_t = train_t[idx:batch_size+idx]\n",
    "    batch_domain = domain[idx:batch_size + idx]\n",
    "    \n",
    "    return torch.tensor(batch_x), torch.tensor(batch_t), torch.tensor(batch_domain)\n",
    "\n",
    "def mkBatch(train_x, train_t, domain, batch_size):\n",
    "    b_x = []\n",
    "    b_t = []\n",
    "    d = []\n",
    "    \n",
    "    for i in range(int(train_x.shape[0]/batch_size)):\n",
    "        b_x.append(train_x[i*batch_size: batch_size*(i+1), :, :])\n",
    "        b_t.append(train_t[i*batch_size: batch_size*(i+1)])\n",
    "        d.append(domain[i*batch_size: batch_size*(i+1)])\n",
    "\n",
    "    return b_x, b_t, d\n",
    "\n",
    "import random\n",
    "def rand_ints_nodup(a, b, k):\n",
    "  ns = []\n",
    "  while len(ns) < k:\n",
    "    n = random.randint(a, b)\n",
    "    if not n in ns:\n",
    "      ns.append(n)\n",
    "  return ns   \n",
    "   \n",
    "from visdom import Visdom\n",
    "#X_train_1, X_test_1, Y_train_1, Y_test_1, domain_train_1, domain_test_1 = setData_imp(de_1_1, label_data, 50)\n",
    "\n",
    "def main():\n",
    "    #viz = Visdom()  # インスタンスを作る\n",
    "    \n",
    "    X_train_1, X_test_1, Y_train_1, Y_test_1, domain_train_1, domain_test_1 = setData_imp(de_1_1, label_data, 50)\n",
    "   \n",
    "    training_size = X_train_1.shape[0]\n",
    "    test_size = X_test_1.shape[0]\n",
    "    epochs_num = 30\n",
    "    hidden_size = 64\n",
    "    batch_size = 32\n",
    "    target_dim = 3\n",
    "   \n",
    "    \n",
    "    model = DANN_Bi(310, hidden_size, target_dim)\n",
    "    model.double()\n",
    "    \n",
    "    #summary(model, input_size=((100, 50, 310), 1))\n",
    "    \n",
    "    #すでにsoftmaxとsigmoidされている\n",
    "    class_loss_function = nn.CrossEntropyLoss()\n",
    "    domain_loss_function = nn.BCEWithLogitsLoss()\n",
    "   \n",
    "    optimizer = optim.Adam(model.parameters(), lr = 0.001, weight_decay = 0.0001)\n",
    "    \n",
    "    \n",
    "    model.train()#学習モード\n",
    "\n",
    "    pltloss = np.array([0])\n",
    "    pltclass = np.array([0])\n",
    "    pltdomain = np.array([0])\n",
    "    e = np.arange(1, epochs_num+1)\n",
    "    \n",
    "    b_x, b_t, d = mkBatch(X_train_1, Y_train_1, domain_train_1, batch_size)\n",
    "    b_x_t, b_t_t, d_t = mkBatch(X_test_1, Y_test_1, domain_test_1, batch_size)\n",
    "    for epoch in range(epochs_num):\n",
    "        #training\n",
    "        running_loss = 0.0\n",
    "        training_accuracy = 0.0\n",
    "        label_eval = []\n",
    "        class_eval = []\n",
    "        d_label_eval = []\n",
    "        domain_eval = []\n",
    "        \n",
    "        test_eval = []\n",
    "        \n",
    "        running_class_loss = 0.0\n",
    "        running_domain_loss = 0.0\n",
    "        \n",
    "        m = 0\n",
    "        k = 0\n",
    "        \n",
    "        rand = rand_ints_nodup(0, len(b_x)-1, len(b_x))\n",
    "        rand_t = rand_ints_nodup(0, len(b_x_t)-1, len(b_x_t))\n",
    "        \n",
    "        for i in range(int(training_size/batch_size)):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            \n",
    "            #GRLのalpha\n",
    "            p = float(i + epoch * training_size) / epochs_num / training_size\n",
    "            alpha = 2. / (1. + np.exp(-10 * p)) - 1\n",
    "            alpha = 1.0\n",
    "            \n",
    "            #バッチデータ取得\n",
    "            data, label, domain = torch.tensor(b_x[rand[i]]), torch.tensor(b_t[rand[i]]), torch.tensor(d[rand[i]])\n",
    "            \n",
    "            #モデルに入力\n",
    "            class_output, domain_output = model(data, alpha)\n",
    "            \n",
    "            #ラベル分類の損失\n",
    "            class_loss = class_loss_function(class_output, label)\n",
    "           \n",
    "            \n",
    "            #ドメインサイズ成形\n",
    "            #domain = domain.view(domain.size(0), 1)\n",
    "           \n",
    "            #ドメイン分類の損失\n",
    "            #domain_loss = domain_loss_function(domain_output, domain.float())\n",
    "            domain_loss = class_loss_function(domain_output, domain)\n",
    "            \n",
    "            #テストデータの取得\n",
    "            data_t, label_t, domain_t = mkRandomBatch(X_test_1, Y_test_1, domain_test_1, batch_size)\n",
    "            \n",
    "            #テストデータのドメイン分類の出力を取得\n",
    "            _, domain_t_output = model(data_t, alpha)\n",
    "            #print(domain_t_output)\n",
    "            \n",
    "            #domain_t = domain_t.view(domain_t.size(0), 1)\n",
    "            \n",
    "            #テストデータのドメインの損失を取得\n",
    "            #domain_t_loss = domain_loss_function(domain_t_output, domain_t.float())\n",
    "            domain_t_loss = class_loss_function(domain_t_output, domain_t)\n",
    "            \n",
    "            #クラスラベル分類の損失，ドレーニングドメインラベル分類の損失，テストドメインラベル分類の損失を一つに\n",
    "            err = class_loss + domain_loss + domain_t_loss\n",
    "            \n",
    "            err.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "            running_loss += err.item()\n",
    "            running_class_loss += class_loss.item()\n",
    "            running_domain_loss += domain_loss.item()\n",
    "            \n",
    "            \n",
    "            for j in range(class_output.shape[0]):\n",
    "                m += 1\n",
    "                if epoch == epochs_num - 1:\n",
    "                    label_eval.append(label[j])\n",
    "                    class_eval.append(torch.argmax(class_output[j, :]))\n",
    "                    \n",
    "                if torch.argmax(class_output[j, :]) == label[j]:\n",
    "                    training_accuracy += 1\n",
    "                    \n",
    "            for j in range(domain_output.shape[0]):\n",
    "                if epoch == epochs_num - 1:\n",
    "                    d_label_eval.append(domain[j])\n",
    "                    domain_eval.append(torch.argmax(domain_output[j, :]))\n",
    "                    #if domain_output[j] > 0:\n",
    "                     #   domain_eval.append(1)\n",
    "                    #else:\n",
    "                     #   domain_eval.append(0)\n",
    "                        \n",
    "            for j in range(domain_t_output.shape[0]):\n",
    "                if epoch == epochs_num - 1:\n",
    "                    d_label_eval.append(domain_t[j])\n",
    "                    domain_eval.append(torch.argmax(domain_t_output[j, :]))\n",
    "                    #if domain_t_output[j] > 0:\n",
    "                     #   domain_eval.append(1)\n",
    "                        \n",
    "                    #else:\n",
    "                     #   domain_eval.append(0)\n",
    "            \n",
    "            \n",
    "            \n",
    "       \n",
    "        if epoch == 0:\n",
    "            pltloss = np.array([running_loss])\n",
    "            pltclass = np.array([running_class_loss])\n",
    "            pltdomain = np.array([running_domain_loss])\n",
    "        else:\n",
    "            pltloss = np.append(pltloss, running_loss)\n",
    "            pltclass = np.append(pltclass, running_class_loss)\n",
    "            pltdomain = np.append(pltdomain, running_domain_loss)\n",
    "            \n",
    "            \n",
    "        #viz.line(X=np.array([epoch]), Y=np.array([running_class_loss]), win='loss', name='class_loss', update='append')\n",
    "        #viz.line(X=np.array([epoch]), Y=np.array([running_domain_loss]), win='loss', name='domain_acc', update='append')\n",
    "        model.eval()\n",
    "        l = 0\n",
    "        test_accuracy2 = 0.0\n",
    "        class_eval_m = []\n",
    "        label_eval_m = []\n",
    "        for i in range(int(test_size/batch_size)):\n",
    "            p = float(i + epoch * test_size) / epochs_num / test_size\n",
    "            alpha = 2. / (1. + np.exp(-10 * p)) - 1\n",
    "            alpha = 1.0\n",
    "    \n",
    "            data, label, domain = torch.tensor(b_x_t[rand_t[i]]), torch.tensor(b_t_t[rand_t[i]]), torch.tensor(d_t[rand_t[i]])\n",
    "            class_t_output ,domain_t_output = model(data, alpha)\n",
    "             \n",
    "            for j in range(class_t_output.shape[0]):\n",
    "                l += 1\n",
    "                if epoch == epochs_num - 1:\n",
    "                    label_eval_m.append(label[j])\n",
    "                    class_eval_m.append(torch.argmax(class_t_output[j, :]))\n",
    "                if torch.argmax(class_t_output[j, :]) == label[j]:\n",
    "                    test_accuracy2 += 1\n",
    "        \n",
    "        test_accuracy2 /= l\n",
    "        \n",
    "        training_accuracy /= m\n",
    "        test_eval.append(test_accuracy2)\n",
    "        \n",
    "        print('%d loss: %.3f, training_accuracy: %.5f, test_accuracy: %.5f' % (epoch + 1, running_loss, training_accuracy, test_accuracy2))\n",
    "   \n",
    "    \n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "      \n",
    "    k = 0\n",
    "    b_x, b_t, d = mkBatch(X_test_1, Y_test_1, domain_test_1, batch_size)\n",
    "    rand = rand_ints_nodup(0, len(b_x)-1, len(b_x))\n",
    "    test_accuracy = 0.0\n",
    "    class_eval_t = []\n",
    "    label_eval_t = []\n",
    "    for i in range(int(test_size/batch_size)):\n",
    "        p = float(i + epoch * test_size) / epochs_num / test_size\n",
    "        alpha = 2. / (1. + np.exp(-10 * p)) - 1\n",
    "        alpha = 1.0\n",
    "        \n",
    "        data, label, domain = torch.tensor(b_x[rand[i]]), torch.tensor(b_t[rand[i]]), torch.tensor(d[rand[i]])\n",
    "        class_t_output ,domain_t_output = model(data, alpha)\n",
    "             \n",
    "        for j in range(class_t_output.shape[0]):\n",
    "            k += 1\n",
    "            if epoch == epochs_num - 1:\n",
    "                label_eval_t.append(label[j])\n",
    "                class_eval_t.append(torch.argmax(class_t_output[j, :]))\n",
    "            if torch.argmax(class_t_output[j, :]) == label[j]:\n",
    "                test_accuracy += 1\n",
    "       \n",
    "    test_accuracy /= k\n",
    "    \n",
    "    print('training_accuracy: %.5f' % (test_accuracy))\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    c1, c2, c3 = \"blue\", \"green\", \"red\"\n",
    "    l1, l2, l3 = \"class\", \"domain\", \"all\"\n",
    "    \n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('Model loss')\n",
    "    ax.grid()\n",
    "    \n",
    "    ax.plot(e, pltclass, color = c1, label = l1)\n",
    "    ax.plot(e, pltdomain, color = c2, label = l2)\n",
    "    #ax.plot(e,pltloss, color = c3, label = l3)\n",
    "    ax.legend(loc = 0)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    plt.plot(e, pltloss)   \n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    plt.plot(len(test_accuracy2), test_accuracy2)   \n",
    "    plt.title('Test Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    plt.plot(e, pltclass)   \n",
    "    plt.title('class loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(e, pltdomain)   \n",
    "    plt.title('domain loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()\n",
    "    \"\"\"\n",
    "           \n",
    "    c = confusion_matrix(label_eval, class_eval)\n",
    "    print(c)\n",
    "    sns.heatmap(c, annot = True, fmt = 'g', square = True)\n",
    "    plt.title(\"training\")\n",
    "    plt.xlabel(\"predict\")\n",
    "    plt.ylabel(\"true\")\n",
    "    plt.show()\n",
    "    \n",
    "    cd = confusion_matrix(d_label_eval, domain_eval)\n",
    "    print(cd)\n",
    "    sns.heatmap(cd, annot = True, fmt = 'g', square = True)\n",
    "    plt.title(\"domain_eval\")\n",
    "    plt.xlabel(\"predict\")\n",
    "    plt.ylabel(\"true\")\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    ct = confusion_matrix(label_eval_m, class_eval_m)\n",
    "    print(ct)\n",
    "    sns.heatmap(ct, annot = True, fmt = 'g', square = True)\n",
    "    plt.title(\"test\")\n",
    "    plt.xlabel(\"predict\")\n",
    "    plt.ylabel(\"true\")\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
